#########################################
# LIEBERMAN LAB SNAKEFILE FOR CASE STEP #
#########################################

# Version History:
# # 2022.03 Martin: Snakemake can now deal with multiple reference genomes at once
# # 2020.01 Felix: Added build_candidate_mutation_table.py incl. output of whole-genome coverage matrix and double-normalized matrix.
# # 2019.02.12, Arolyn: rule candidate_mutation_table now calling make_candidate_mutation_table_snakemake_indels.m instead of make_candidate_mutation_table_snakemake.m
# # 2018.12, Felix/Arolyn: Original Snakefile from the lab hackathon

# Reminders:
# # Put your own email address in cluster.slurm.json so that you get emails about failures. No one else wants your slurm emails.

import sys,os,subprocess
SCRIPTS_DIRECTORY = "./scripts"
sys.path.insert(0, SCRIPTS_DIRECTORY)

## USER defined variables (in theory do not need to be touched)
spls = "samples_case.csv"
maxFQ = -30 # purity threshold (from mapping quality) for including position

## pre-snakemake
from case_modules import *

''' PRE-SNAKEMAKE '''
## define couple of lists from samples_case.csv
## FORMAT: Path,Sample,ReferenceGenome,Outgroup
# NOTE: case analysis expects:
#   - unique sample IDs
#   - Col1==Paths, points to the snakemake directory used for mapping, not raw data! > only single entries accepted!
[PATH_ls, SAMPLE_ls, REF_Genome_ls, OUTGROUP_ls] = read_samples_caseCSV(spls)

## Te following lines are essential to run the snakemake with multiple reference genomes and get a candidate mutation table for each reference genome
## generate two dictories, which have as key the unique reference geneome for all samples and their outgroup tags 
SAMPLE_per_ref_dict = unique_key_generator(REF_Genome_ls, SAMPLE_ls)
OUTGROUP_per_ref_dict = unique_key_generator(REF_Genome_ls, OUTGROUP_ls)


''' SNAKEMAKE '''
rule all:
    input:
        # # Data links only # #
        # expand("data/vcf/{sampleID}_ref_{references}_outgroup{outgroup}.vcf.gz",zip,sampleID=SAMPLE_ls, references=REF_Genome_ls, outgroup=OUTGROUP_ls),
        # expand("data/qual/{sampleID}_ref_{references}_outgroup{outgroup}.quals.mat",zip,sampleID=SAMPLE_ls, references=REF_Genome_ls, outgroup=OUTGROUP_ls),
        # expand("data/diversity/{sampleID}_ref_{references}_outgroup{outgroup}.diversity.mat",zip,sampleID=SAMPLE_ls, references=REF_Genome_ls, outgroup=OUTGROUP_ls),
        # # Everything # #
        expand("2-candidate_mutation_table/{references}/candidate_mutation_table.pickle.gz",references=set(REF_Genome_ls)), ## for each unique entry (set()) the output is expected
        # # Including cleanup step # #
        "logs/DONE_cleanUp"


rule build_data_links:
    output:
        vcf_links = expand("data/vcf/{sampleID}_ref_{references}_outgroup{outgroup}.vcf.gz",zip,sampleID=SAMPLE_ls, references=REF_Genome_ls, outgroup=OUTGROUP_ls),
        qual_mat_links = expand("data/qual/{sampleID}_ref_{references}_outgroup{outgroup}.quals.npz",zip,sampleID=SAMPLE_ls, references=REF_Genome_ls, outgroup=OUTGROUP_ls),
        div_mat_links = expand("data/diversity/{sampleID}_ref_{references}_outgroup{outgroup}.diversity.npz",zip,sampleID=SAMPLE_ls, references=REF_Genome_ls, outgroup=OUTGROUP_ls),
    log:
        "logs/build_links.log",
    run:
        import subprocess
        subprocess.run( "rm -fr data/ " ,shell=True) # clean it up prior run
        subprocess.run( "mkdir -p data/vcf/ data/qual/ data/diversity/ " ,shell=True)
        for i in range(len(SAMPLE_ls)):
            subprocess.run( "ln -fs -T " + PATH_ls[i] + "/6-diversity/" + SAMPLE_ls[i] + "_ref_" + REF_Genome_ls[i] + "*diversity.npz data/diversity/" + SAMPLE_ls[i] + "_ref_" + REF_Genome_ls[i] + "_outgroup" + OUTGROUP_ls[i] + ".diversity.npz" ,shell=True)
            subprocess.run( "ln -fs -T " + PATH_ls[i] + "/5-quals/" + SAMPLE_ls[i] + "_ref_" + REF_Genome_ls[i] + "*quals.npz data/qual/" + SAMPLE_ls[i] + "_ref_" + REF_Genome_ls[i] + "_outgroup" + OUTGROUP_ls[i] + ".quals.npz" ,shell=True)
            subprocess.run( "ln -fs -T " + PATH_ls[i] + "/4-vcf/" + SAMPLE_ls[i] + "_ref_" + REF_Genome_ls[i] + "*variant.vcf.gz data/vcf/" + SAMPLE_ls[i] + "_ref_" + REF_Genome_ls[i] + "_outgroup" + OUTGROUP_ls[i] + ".vcf.gz " ,shell=True)


rule variants2positions:
    input:
        variants = "data/vcf/{sampleID}_ref_{references}_outgroup{outgroup}.vcf.gz",
    params:
        REF_GENOME_DIRECTORY = "/nexus/posix0/MPIIB-keylab/reference_genomes/{references}/",
        outgroup_tag = "{outgroup}", # boolean (0==ingroup or 1==outgroup)
    output:
        mat_positions = "1-temp_pos/{sampleID}_ref_{references}_outgroup{outgroup}_positions.npz",
    group:
        "var2pos",
    run:
        ## Might need to load os and insert script dir at first
        generate_positions_single_sample_snakemake_withoutbool(input.variants, output.mat_positions, maxFQ, params.REF_GENOME_DIRECTORY, params.outgroup_tag)


rule combine_positions_prep:
    input:
        ## combination steps are done per reference genome, double expand is therefore needed!
        mat_positions = lambda wildcards: expand(expand("1-temp_pos/{{sampleID}}_ref_{references}_outgroup{{outgroup}}_positions.npz",references=wildcards.references),zip,sampleID=SAMPLE_per_ref_dict[wildcards.references],outgroup=OUTGROUP_per_ref_dict[wildcards.references]), ## Expand at first over the wildcard reference genome and mask the other two wildcarfs (done with the '{{}}'. Then, as just the zipped output is expected, zip the other two wildcards together
        #mat_positions = expand("1-temp_pos/{sampleID}_ref_{references}_outgroup{outgroup}_positions.npz",zip,sampleID=SAMPLE_ls, references=REF_Genome_ls, outgroup=OUTGROUP_ls)
    output:
        string_input_p_positions = "1-temp_pos/{references}/string_file_other_p_to_consider.txt",
    group:
        "pre_cand_mut_table",
    run:
        with open(output.string_input_p_positions, "w") as f:
            print(*input.mat_positions, sep=" ", file=f)


# build input for candidate_mutation_table
rule string_outgroup_bool:
    input:
      rules.combine_positions_prep.output.string_input_p_positions, ## Needed to streamline rules and be able to group them on hpc cluster per ref genome
    params:
      outgroup_bool = lambda wildcards:  expand( "{outgroup}" , outgroup=OUTGROUP_per_ref_dict[wildcards.references] ), ## expand for each reference genome individually and store in separate output files
    output:
      string_outgroup_bool = "1-temp_pos/{references}/string_outgroup_bool.txt",
    group:
      "pre_cand_mut_table",
    run:
      with open(output.string_outgroup_bool, "w") as f:
        print(*params.outgroup_bool, sep=" ", file=f)


rule combine_positions:
    input:
        string_input_p_positions = "1-temp_pos/{references}/string_file_other_p_to_consider.txt",
        string_outgroup_bool = "1-temp_pos/{references}/string_outgroup_bool.txt",
    params:
        file_other_p_to_consider = "add_positions/other_positions.npz", ## candidate positions which could be added manually 
        REF_GENOME_DIRECTORY = "/nexus/posix0/MPIIB-keylab/reference_genomes/{references}/"
    output:
        mat_positions = "1-temp_pos/{references}/allpositions.npz",
    group:
        "pre_cand_mut_table",
    run:
        combine_positions_snakemake_out(input.string_input_p_positions, params.file_other_p_to_consider, output.mat_positions, input.string_outgroup_bool, params.REF_GENOME_DIRECTORY, maxFQ)


# build input for candidate_mutation_table
rule string_diversity_mat:
    input:
        cluster_in = rules.combine_positions.output.mat_positions, ## Needed to streamline rules and be able to group them on hpc cluster per ref genome
        diversity_mat = lambda wildcards: expand(expand("data/diversity/{{sampleID}}_ref_{references}_outgroup{{outgroup}}.diversity.npz",references=wildcards.references),zip,sampleID=SAMPLE_per_ref_dict[wildcards.references],outgroup=OUTGROUP_per_ref_dict[wildcards.references]),
        #diversity_mat = expand("data/diversity/{sampleID}_ref_{references}_outgroup{outgroup}.diversity.npz",zip,sampleID=SAMPLE_ls, references=REF_Genome_ls, outgroup=OUTGROUP_ls),
    output:
        string_diversity_mat = "1-temp_pos/{references}/string_diversity_mat.txt",
    group:
        "pre_cand_mut_table",
    run:
        with open(output.string_diversity_mat, "w") as f:
            print(*input.diversity_mat, sep=" ", file=f)


# build input for candidate_mutation_table
rule string_quals_mat:
    input:
      cluster_in = rules.string_diversity_mat.output.string_diversity_mat, ## Needed to streamline rules and be able to group them on hpc cluster per ref genome
      quals_mat = lambda wildcards: expand(expand("data/qual/{{sampleID}}_ref_{references}_outgroup{{outgroup}}.quals.npz",references=wildcards.references),zip,sampleID=SAMPLE_per_ref_dict[wildcards.references],outgroup=OUTGROUP_per_ref_dict[wildcards.references]),
      #quals_mat = expand("data/qual/{sampleID}_ref_{references}_outgroup{outgroup}.quals.npz",zip,sampleID=SAMPLE_ls, references=REF_Genome_ls, outgroup=OUTGROUP_ls),
    output:
      string_qual_mat = "1-temp_pos/{references}/string_qual_mat.txt",
    group:
      "pre_cand_mut_table",
    run:
      with open(output.string_qual_mat, "w") as f:
        print(*input.quals_mat, sep=" ", file=f)



# build input for candidate_mutation_table
## cannot be grouped as this is a single independent step
rule string_sampleID_names:
    input:
        rules.string_quals_mat.output.string_qual_mat, ## Needed to streamline rules and be able to group them on hpc cluster per ref genome
    params:
        sampleID_names = lambda wildcards: expand( "{sampleID}" , sampleID=SAMPLE_per_ref_dict[wildcards.references] ), ## expand for each reference genome individually and store in separate output files
    output:
        string_sampleID_names = "1-temp_pos/{references}/string_sampleID_names.txt",
    group:
        "pre_cand_mut_table",
    run:
        with open(output.string_sampleID_names, "w") as f:
                print(*params.sampleID_names, sep=" ", file=f)


rule candidate_mutation_table:
    input:
        mat_positions = "1-temp_pos/{references}/allpositions.npz",
        string_diversity_mat = "1-temp_pos/{references}/string_diversity_mat.txt",
        string_qual_mat = "1-temp_pos/{references}/string_qual_mat.txt",
        string_sampleID_names = "1-temp_pos/{references}/string_sampleID_names.txt",
        string_outgroup_bool = "1-temp_pos/{references}/string_outgroup_bool.txt",
    output:
        candidate_mutation_table = "2-candidate_mutation_table/{references}/candidate_mutation_table.pickle.gz",
    group:
        "cand_mut_table_clean",
    conda:
        "envs/py_for_snakemake.yaml",
    shell:
        # -c/-n optional flag to build cov/norm matrix in folder of cmt. check -h for help.
        """
        python3 scripts/build_candidate_mutation_table_npz_script.py -p {input.mat_positions} -s {input.string_sampleID_names} -g {input.string_outgroup_bool} -q {input.string_qual_mat} -d {input.string_diversity_mat} -o {output.candidate_mutation_table} -cn
        """


rule cleanUp:
    input:
        candidate_mutation_table = expand("2-candidate_mutation_table/{references}/candidate_mutation_table.pickle.gz",references=set(REF_Genome_ls)),
    params:
        temp_folder = "1-temp_pos"
    output:
        "logs/DONE_cleanUp"
    group:
        "cand_mut_table_clean",
    shell:
        " rm -rf {params.temp_folder} ; touch {output} "
